{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import namedtuple, defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "from six import add_metaclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixSavingMeta(type):\n",
    "    def __new__(mcs, name, parents, attrs):\n",
    "        new_type = type.__new__(mcs, name, parents, attrs)\n",
    "        prefix = attrs.get(\"prefix_name\")\n",
    "        if prefix:\n",
    "            new_type.prefix_map[prefix] = new_type\n",
    "        short = attrs.get(\"short_prefix\")\n",
    "        if short:\n",
    "            new_type.prefix_map[short] = new_type\n",
    "        return new_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagTypeEnum(Enum):\n",
    "    unimod = 0\n",
    "    psimod = 1\n",
    "    massmod = 2\n",
    "    generic = 3\n",
    "    info = 4\n",
    "    gnome = 5\n",
    "    formula = 6\n",
    "    glycan = 7\n",
    "    xlmod = 8\n",
    "    localization_marker = 9\n",
    "    group_placeholder = 999\n",
    "    \n",
    "\n",
    "@add_metaclass(PrefixSavingMeta)\n",
    "class TagBase(object):\n",
    "    __slots__ = (\"type\", \"value\", \"extra\", \"group_id\")\n",
    "\n",
    "    prefix_name = None\n",
    "    short_prefix = None\n",
    "    prefix_map = {}\n",
    "    \n",
    "    def __init__(self, type, value, extra=None, group_id=None):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "        self.extra = extra or []\n",
    "        self.group_id = group_id\n",
    "\n",
    "    def __str__(self):\n",
    "        part = self._format_main()\n",
    "        if self.extra:\n",
    "            rest = [str(e) for e in self.extra]\n",
    "            label = '|'.join([part] + rest)\n",
    "        else:\n",
    "            label = part\n",
    "        if self.group_id:\n",
    "            label = '%s#%s' % (label, self.group_id)\n",
    "        return label\n",
    "    \n",
    "    def __repr__(self):\n",
    "        template = \"{self.__class__.__name__}({self.value!r}, {self.extra!r}, {self.group_id!r})\"\n",
    "        return template.format(self=self)\n",
    "\n",
    "\n",
    "class LocalizationMarker(TagBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __init__(self, value, extra=None, group_id=None):\n",
    "        assert group_id is not None\n",
    "        super(LocalizationMarker, self).__init__(TagTypeEnum.localization_marker, float(value), extra, group_id)\n",
    "    \n",
    "    def _format_main(self):\n",
    "        return \"#{self.group_id}({self.value!f})\".format(self=self)\n",
    "    \n",
    "    \n",
    "class MassModification(TagBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __init__(self, value, extra=None, group_id=None):\n",
    "        super(MassModification, self).__init__(TagTypeEnum.massmod, float(value), extra, group_id)\n",
    "    \n",
    "    def _format_main(self):\n",
    "        return '%0.4f' % self.value\n",
    "\n",
    "    \n",
    "class ControlledVocabularyModificationBase(TagBase):\n",
    "    _tag_type = None\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __init__(self, value, extra=None, group_id=None):\n",
    "        super(ControlledVocabularyModificationBase, self).__init__(\n",
    "            self._tag_type, value, extra, group_id)\n",
    "\n",
    "    def _format_main(self):\n",
    "        return \"{self.prefix_name}:{self.value}\".format(self=self)\n",
    "\n",
    "    \n",
    "class GenericModification(TagBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __init__(self, value, extra=None, group_id=None):\n",
    "        super(GenericModification, self).__init__(TagTypeEnum.generic, value, extra, group_id)\n",
    "    \n",
    "    def _format_main(self):\n",
    "        return self.value\n",
    "    \n",
    "\n",
    "class UnimodModification(ControlledVocabularyModificationBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    prefix_name = \"UNIMOD\"\n",
    "    short_prefix = \"U\"\n",
    "    _tag_type = TagTypeEnum.unimod\n",
    "\n",
    "\n",
    "class PSIModModification(ControlledVocabularyModificationBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    prefix_name = \"MOD\"\n",
    "    short_prefix = 'M'\n",
    "    _tag_type = TagTypeEnum.psimod\n",
    "\n",
    "\n",
    "class GNOmeModification(ControlledVocabularyModificationBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    prefix_name = \"GNO\"\n",
    "    short_prefix = 'G'\n",
    "    _tag_type = TagTypeEnum.gnome\n",
    "\n",
    "    \n",
    "class XLMODModification(ControlledVocabularyModificationBase):\n",
    "    __slots__ = ()\n",
    "    \n",
    "    prefix_name = \"XLMOD\"\n",
    "#     short_prefix = 'XL'\n",
    "    _tag_type = TagTypeEnum.xlmod\n",
    "    \n",
    "    \n",
    "class TagParserStateEnum(Enum):\n",
    "    start = 0\n",
    "    group_id = 1\n",
    "\n",
    "def split_tags(tokens):\n",
    "    starts = [0]\n",
    "    ends = []\n",
    "    for i, c in enumerate(tokens):\n",
    "        if c == '|':\n",
    "            ends.append(i)\n",
    "            starts.append(i + 1)\n",
    "    ends.append(len(tokens))\n",
    "    out = []\n",
    "    for i, start in enumerate(starts):\n",
    "        end = ends[i]\n",
    "        out.append(tokens[start:end])\n",
    "    return out\n",
    "\n",
    "def find_prefix(tokens):\n",
    "    for i, c in enumerate(tokens):\n",
    "        if c == ':':\n",
    "            return ''.join(tokens[:i]), ''.join(tokens[i + 1:])\n",
    "    return None, tokens\n",
    "    \n",
    "def process_tag_tokens(tokens):\n",
    "    parts = split_tags(tokens)\n",
    "    main_tag = parts[0]\n",
    "    if main_tag[0] in ('+', '-'):\n",
    "        main_tag = ''.join(main_tag)\n",
    "        main_tag = MassModification(main_tag)\n",
    "    else:\n",
    "        prefix, value = find_prefix(main_tag)\n",
    "        if prefix is None:\n",
    "            main_tag = GenericModification(''.join(value))\n",
    "        else:\n",
    "            tag_type = TagBase.prefix_map[prefix]\n",
    "            main_tag = tag_type(value)\n",
    "    if len(parts) > 1:\n",
    "        extras = []\n",
    "        for part in parts:\n",
    "            prefix, value = find_prefix(part)\n",
    "            if prefix is None:\n",
    "                if value.startswith(\"#\"):\n",
    "                    main_tag.group_id = value\n",
    "                else:\n",
    "                    main_tag.extra.append(GenericModification(''.join(value)))\n",
    "            else:\n",
    "                tag_type = TagBase.prefix_map[prefix]\n",
    "                main_tag.extra.append(tag_type(value))\n",
    "    return main_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QWERTYIPASDFGHKLCVNM'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyteomics import parser\n",
    "''.join(parser.std_amino_acids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserStateEnum(Enum):\n",
    "    before_sequence = 0\n",
    "    tag_before_sequence = 1\n",
    "    global_tag = 2\n",
    "    fixed_spec = 3\n",
    "    labile_tag = 4\n",
    "    sequence = 5\n",
    "    tag_in_sequence = 6\n",
    "    interval_tag = 7\n",
    "    tag_after_sequence = 8\n",
    "    \n",
    "    done = 999\n",
    "\n",
    "\n",
    "BEFORE = ParserStateEnum.before_sequence\n",
    "TAG_BEFORE = ParserStateEnum.tag_before_sequence\n",
    "FIXED = ParserStateEnum.fixed_spec\n",
    "GLOBAL = ParserStateEnum.global_tag\n",
    "LABILE = ParserStateEnum.labile_tag\n",
    "SEQ = ParserStateEnum.sequence\n",
    "TAG = ParserStateEnum.tag_in_sequence\n",
    "INTERVAL_TAG = ParserStateEnum.interval_tag\n",
    "TAG_AFTER = ParserStateEnum.tag_after_sequence\n",
    "DONE = ParserStateEnum.done\n",
    "\n",
    "VALID_AA = set(\"QWERTYIPASDFGHKLCVNM\")\n",
    "\n",
    "def tokenize_proforma(sequence):\n",
    "    labile_modifications = []\n",
    "    fixed_modifications = []\n",
    "    unlocalized_modifications = []\n",
    "    intervals = []\n",
    "    isotopes = []\n",
    "    \n",
    "    n_term = None\n",
    "    c_term = None\n",
    "    \n",
    "    i = 0\n",
    "    n = len(sequence)\n",
    "    \n",
    "    positions = []\n",
    "    state = BEFORE\n",
    "    depth = 0\n",
    "    \n",
    "    current_aa = None\n",
    "    current_tag = []\n",
    "    current_interval = None\n",
    "    \n",
    "    while i < n:\n",
    "        c = sequence[i]\n",
    "        i += 1\n",
    "        if state == BEFORE:\n",
    "            if c == '[':\n",
    "                state = TAG_BEFORE\n",
    "                depth = 1\n",
    "            elif c == '{':\n",
    "                state = LABILE\n",
    "                depth = 1\n",
    "            elif c == '<':\n",
    "                state = FIXED\n",
    "            elif c in VALID_AA:\n",
    "                current_aa = c\n",
    "                state = SEQ\n",
    "            elif c == '?':\n",
    "                if current_tag:\n",
    "                    unlocalized_modifications.append(process_tag_tokens(current_tag))\n",
    "                    current_tag = []\n",
    "                else:\n",
    "                    raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "            elif c == '-':\n",
    "                if current_tag:\n",
    "                    n_term = process_tag_tokens(current_tag)\n",
    "                    current_tag = []\n",
    "                else:\n",
    "                    raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "            else:\n",
    "                raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "        elif state == SEQ:\n",
    "            if c in VALID_AA:\n",
    "                positions.append((current_aa, process_tag_tokens(current_tag) if current_tag else None))\n",
    "                current_aa = c\n",
    "                current_tag = []\n",
    "            elif c == '[':\n",
    "                state = TAG\n",
    "                depth = 1\n",
    "            elif c == '(':\n",
    "                current_interval = [len(positions), None, None]\n",
    "            elif c == ')':\n",
    "                if current_interval is None:\n",
    "                    raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "                else:\n",
    "                    current_interval[1] = len(positions)\n",
    "                    if i >= n or sequence[i] != '[':\n",
    "                        raise Exception(\"Missing Interval Tag\")\n",
    "                    i += 1\n",
    "                    depth = 1\n",
    "                    state = INTERVAL_TAG\n",
    "            elif c == '-':\n",
    "                state = TAG_AFTER\n",
    "                if i >= n or sequence[i] != '[':\n",
    "                    raise Exception(\"Missing Interval Tag\")\n",
    "                i += 1\n",
    "                depth = 1                \n",
    "            else:\n",
    "                raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "        elif state == TAG or state == TAG_BEFORE or state == TAG_AFTER:\n",
    "            if c == '[':\n",
    "                depth += 1\n",
    "            elif c == ']':\n",
    "                depth -= 1\n",
    "                if depth <= 0:\n",
    "                    depth = 0\n",
    "                    if state == TAG: \n",
    "                        state = SEQ\n",
    "                    elif state == TAG_BEFORE:\n",
    "                        state = BEFORE\n",
    "                    elif state == TAG_AFTER:\n",
    "                        c_term = process_tag_tokens(current_tag)\n",
    "                        state = DONE\n",
    "            else:\n",
    "                current_tag.append(c)\n",
    "        elif state == LABILE:\n",
    "            if c == '{':\n",
    "                depth += 1\n",
    "            elif c == '}':\n",
    "                depth -= 1\n",
    "                if depth <= 0:\n",
    "                    depth = 0\n",
    "                    labile_modifications.append(process_tag_tokens(current_tag))\n",
    "                    current_tag = []\n",
    "                    state = BEFORE\n",
    "            else:\n",
    "                current_tag.append(c)\n",
    "        elif state == INTERVAL_TAG:\n",
    "            if c == '[':\n",
    "                depth += 1\n",
    "            elif c == ']':\n",
    "                depth -= 1\n",
    "                if depth <= 0:\n",
    "                    depth = 0\n",
    "                    current_interval[2] = process_tag_tokens(current_tag)\n",
    "                    current_tag = []\n",
    "                    intervals.append(current_interval)\n",
    "                    current_interval = None\n",
    "                    state = SEQ\n",
    "            else:\n",
    "                current_tag.append(c)\n",
    "        else:\n",
    "            raise Exception(\"Error In State {state}, unexpected {c} found at index {i}\".format(**locals()))\n",
    "    if current_aa:\n",
    "        positions.append((current_aa, process_tag_tokens(current_tag) if current_tag else None))\n",
    "    return positions, {\n",
    "        'n_term': n_term,\n",
    "        'c_term': c_term,\n",
    "        'unlocalized_modifications': unlocalized_modifications,\n",
    "        'labile_modifications': labile_modifications,\n",
    "        'intervals': intervals,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('S', None),\n",
       "  ('T', UnimodModification('Ox', [], None)),\n",
       "  ('E', None),\n",
       "  ('P', None),\n",
       "  ('P', None),\n",
       "  ('I', None),\n",
       "  ('N', None),\n",
       "  ('G', None)],\n",
       " {'n_term': GenericModification('Hex', [], None),\n",
       "  'c_term': None,\n",
       "  'unlocalized_modifications': [],\n",
       "  'labile_modifications': [GenericModification('Foo', [], None)],\n",
       "  'intervals': [[1, 4, MassModification(18.0, [], None)]]})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq, fields = tokenize_proforma(\"{Foo}[Hex]-ST[U:Ox](EPP)[+18]ING\")\n",
    "seq, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
